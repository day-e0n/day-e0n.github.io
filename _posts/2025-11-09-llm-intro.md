---
title: "LLM ê¸°ì´ˆ - Transformer ì•„í‚¤í…ì²˜ ì´í•´í•˜ê¸°"
date: 2025-11-09 10:00:00 +0900
categories: [LLM]
tags: [ì¸ê³µì§€ëŠ¥, ë”¥ëŸ¬ë‹, Transformer, GPT]
---

## Large Language Models (LLM)

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì€ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ í•™ìŠµëœ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì…ë‹ˆë‹¤.

### Transformer ì•„í‚¤í…ì²˜

TransformerëŠ” "Attention is All You Need" ë…¼ë¬¸ì—ì„œ ì†Œê°œëœ í˜ì‹ ì ì¸ êµ¬ì¡°ì…ë‹ˆë‹¤.

#### Self-Attention ë©”ì»¤ë‹ˆì¦˜

```python
import torch
import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)
    
    def forward(self, values, keys, query, mask):
        N = query.shape[0]
        
        # Query, Key, Value ê³„ì‚°
        Q = self.queries(query)
        K = self.keys(keys)
        V = self.values(values)
        
        # Attention scores
        energy = torch.einsum("nqhd,nkhd->nhqk", [Q, K])
        
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))
        
        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)
        
        out = torch.einsum("nhql,nlhd->nqhd", [attention, V])
        return self.fc_out(out)
```

### Attention ìˆ˜ì‹

Self-Attentionì˜ í•µì‹¬ ìˆ˜ì‹:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

ì—¬ê¸°ì„œ:
- $Q$: Query í–‰ë ¬
- $K$: Key í–‰ë ¬
- $V$: Value í–‰ë ¬
- $d_k$: Key ë²¡í„°ì˜ ì°¨ì›

### ì£¼ìš” LLM ëª¨ë¸

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° ìˆ˜ | ê°œë°œì‚¬ | íŠ¹ì§• |
|------|------------|--------|------|
| GPT-4 | ~1.7T | OpenAI | ë©€í‹°ëª¨ë‹¬ ì§€ì› |
| LLaMA 2 | 70B | Meta | ì˜¤í”ˆì†ŒìŠ¤ |
| Claude | - | Anthropic | ì•ˆì „ì„± ê°•í™” |
| Gemini | - | Google | í†µí•© AI í”Œë«í¼ |

### Fine-tuning ë°©ë²•

**1. Full Fine-tuning**
- ëª¨ë“  íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
- ë†’ì€ ì„±ëŠ¥, ë§ì€ ë¦¬ì†ŒìŠ¤ í•„ìš”

**2. LoRA (Low-Rank Adaptation)**
```python
# LoRA ì ìš© ì˜ˆì‹œ
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=8,  # Rank
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
)

model = get_peft_model(base_model, config)
```

**3. Prompt Engineering**
- ëª¨ë¸ ìˆ˜ì • ì—†ì´ í”„ë¡¬í”„íŠ¸ë¡œ ì œì–´
- ê°€ì¥ ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ë°©ë²•

### ì‹¤ì „ í™œìš©

LLMì„ ì‹œìŠ¤í…œ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì— í™œìš©í•˜ëŠ” ë°©ë²•:
- ì½”ë“œ ìƒì„± ë° ë¦¬íŒ©í† ë§
- ë²„ê·¸ íƒì§€ ë° ìˆ˜ì •
- ë¬¸ì„œ ìë™ ìƒì„±
- ì½”ë“œ ë¦¬ë·° ìë™í™”

LLMì€ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ë°”ê¾¸ê³  ìˆìŠµë‹ˆë‹¤! ğŸ¤–âœ¨
